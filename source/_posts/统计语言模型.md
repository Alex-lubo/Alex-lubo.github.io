---
title: 统计语言模型
date: 2018-06-12 17:32:24
tags:
---

从文本、图片、语音和视屏数据中提取挖掘有价值的信息，离不开自然语言处理（NLP）。而统计语言模型(Statistical Language Model)是很重要的一环，它是所有NLP的基础，被广泛的应用于语音识别、机器翻译、分词、词性标注和信息检索等任务。

比如，在语音识别系统中，对于给定的语音段Ｖoice，需要找到一个使得概率<a ><img src="https://latex.codecogs.com/gif.latex?p(Text|Voice)" /></a>最大的文本段Text，　利用Bayes公式，有:

<a><img src="https://latex.codecogs.com/gif.latex?p(Text|Voice)&space;=&space;\frac{p(Voice|Text)p(Text)}{p(Voice)}" /></a>

其中，ｐ(Voice|Text)为声学模型，p(Text)为语言模型。

统计语言模型简单的说，就是用来计算一个句子概率的统计模型。通常基于一个语料库来构建。一个句子有Ｔ个词（w1,w2、...、wT）,则句子的概率就相当于这Ｔ个词按先后顺序同时出现的联合概率为：

<a><img src="https://latex.codecogs.com/gif.latex?p(W)=p(w^{T})=p(w_{1},w_{2},...,w_{T})=p(w_{1})p(w_{2}|w_{1})p(w_{3}|w_{1},w_{2})...p(w_{T}|w_{1},...,w_{T-1})" /></a>

可见，如果字符串长度为T，那么对每一种可能（假如N种）模型的参数数量就是T，所以统计时计算量会非常大，理论计算量就是TN^T。
模型很简单，计算也很简单，就是计算量太大，操作性太差。于是有了很多近似计算这个概率的办法，比如n-gram, 决策树，最大熵模型，最大熵马尔科夫模型，条件随机场，神经网络等。

### n-gram模型
基本思想就是：一个词出现的概率只与它前面的ｎ-1个词相关（Markov假设）。

首先思考<a><img src="https://latex.codecogs.com/gif.latex?p(w_{k}|w_{1},...,w_{k-1})" /></a>的计算。根据Bayes:

<a><img src="https://latex.codecogs.com/gif.latex?p(w_{k}|w_{1},...,w_{k-1})&space;=&space;\frac{p(w_{1},...,w_{k})}{p(w_{1},...,w_{k-1})}" /></a>

当语料库足够大时，可以根据大数定理，近似的计算上式子：
<a><img src="https://latex.codecogs.com/gif.latex?p(w_{k}|w_{1},...,w_{k-1})&space;=&space;\frac{p(w_{1},...,w_{k})}{p(w_{1},...,w_{k-1})}\approx&space;\frac{Count(w_{1},...,w_{k})}{Count(w_{1},...,w_{k-1})}" /></a>

<a><img src="https://latex.codecogs.com/gif.latex?Count(w_{1},...,w_{k})" /></a>和<a><img src="https://latex.codecogs.com/gif.latex?Count(w_{1},...,w_{k-1})" /></a>表示两个词串在语料库中出现的次数。当ｋ很大时，<a><img src="https://latex.codecogs.com/gif.latex?Count(w_{1},...,w_{k})" /></a>和<a><img src="https://latex.codecogs.com/gif.latex?Count(w_{1},...,w_{k-1})" /></a>的统计就会非常费时。

为了简化这种统计，俄罗斯数学家Markov提出了一个假设：那就是一个词出现的概率只与前面固定的数目(比如n)的词相关，那么上述统计可以简化为：
<a><img src="https://latex.codecogs.com/gif.latex?p(w_{k}|w_{1},...,w_{k-1})\approx&space;p(w_{k}|w_{k-n&plus;1},...,w_{k-1})=\frac{p(w_{k-n&plus;1},...,w_{k})}{p(w_{k-n&plus;1},...,w_{k-1})}\approx&space;\frac{Count(w_{k-n&plus;1},...,w_{k-1}}{Count(w_{k-n&plus;1},...,w_{k-1}}" /></a>

那么现在，单个参数的统计变的容易，参数的总数也变少了。语言模型p(Text)的计算可以以一种简单的方式很快计算完成。

### 词向量
将自然语言交个计算机处理，首先需要将语言数字化。词向量就是一种很好的方式。

One-hot Representation是一种，实现就是根据字典中的词，然后对应句子，存在的词值1，不存在的全为0. 它不能很好的刻画词与词之间的相似性，而且容易受维数灾难的困扰。

Distributated Representation是另一种词向量，可以克服One-hot词向量的问题，并可以很好的用在深度学习的场景中。实现方法：将语言中的每个词映射成一个固定长度的短向量，这些向量构成一个词向量空间，向量的距离来判断它们之间的相似性。
